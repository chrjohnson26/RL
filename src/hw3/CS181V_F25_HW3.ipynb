{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSRfymzpKUt3"
      },
      "source": [
        "# HW3 Programming: Finding Optimal Policies with Dynamic Programming\n",
        "\n",
        "Your assignment is to:\n",
        "  * modify this notebook as specified to generate the figures 3.2 and 3.5 in our textbook\n",
        "  \n",
        "\n",
        "I've marked the code cells that you should *not* change with comments. Sections where you have something to do are marked \"TODO.\"\n",
        "\n",
        "You should start by duplicating this notebook:\n",
        "  - To save to your Google Drive, choose _File/Save a Copy in Driveâ€¦_ (from there you can open the file in Colab)\n",
        "  - To save to your local machine, choose _File/Download/Download .ipnb_ (from there you can open the file in your favorite editor for notebooks)\n",
        "\n",
        "When you submit, you will download the _.ipnb_ file and then upload it to Gradescope (details below).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h9ShEq_ldjlm"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVe9Pd0k8-cH"
      },
      "source": [
        "Note that we don't import any modules for random number generation. These dynamic programming techniques make use of probability information, but don't make use of any randomly generated quantities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w5tCI1Ma9CYn"
      },
      "outputs": [],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "GAMMA=0.9 # Throughout this notebook we will use a discount rate of 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39nfhQri9yYe"
      },
      "source": [
        "# MDP Class\n",
        "\n",
        "To begin with, we'll define an abstract `MDP` class, along with a very simple subclass: `SimpleMDP`.\n",
        "\n",
        "An `MDP` represents the environment. It's main job is to provide, for a given state and action pair, the probability distribution for next state and reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_MFsdMcX9dU-"
      },
      "outputs": [],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "class MDP:\n",
        "  \"\"\"Represents an MDP (environment).  Has two instance variables: states and\n",
        "  actions which are arrays of all possible states and actions.\"\"\"\n",
        "  def __init__(self, states, actions):\n",
        "    self.states = states\n",
        "    self.actions = actions\n",
        "\n",
        "  def resultsFor(self, s, a):\n",
        "    \"\"\"Given a state and action, returns a dictionary keyed by (state, reward)\n",
        "    pairs whose value is the probability of that pair.\n",
        "\n",
        "    This is similar to the p(s', r | s, a) function.\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def formatv(self, v):\n",
        "    \"\"\"Returns an easy-to-read representation of the value function.\"\"\"\n",
        "    return str(v)\n",
        "\n",
        "  def formatpi(self, pi):\n",
        "    \"\"\"Returns an easy-to-read representation of the policy.\"\"\"\n",
        "    return str(pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALXTepknYOMJ"
      },
      "source": [
        "# SimpleMDP Class\n",
        "\n",
        "This class is a concrete example of an `MDP` with 2 states and 2 actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "McgCbFs6YByV"
      },
      "outputs": [],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "class SimpleMDP(MDP):\n",
        "  def __init__(self):\n",
        "    super().__init__([\"S1\", \"S2\"], [\"A1\", \"A2\"])\n",
        "    self.p = {\n",
        "        (\"S1\", \"A1\"): {\n",
        "            (\"S1\", 5): 0.25,    # p(S1, 5, S1, A1) = 0.25\n",
        "            (\"S2\", 0): 0.75     # p(S2, 5, S1, A1) = 0.75\n",
        "        },\n",
        "        (\"S1\", \"A2\"): {\n",
        "            (\"S2\", 0): 1,\n",
        "        },\n",
        "        (\"S2\", \"A1\"): {\n",
        "            (\"S2\", 0): 1,\n",
        "        },\n",
        "        (\"S2\", \"A2\"): {\n",
        "            (\"S1\", 3): 0.1,\n",
        "            (\"S2\", 0): 0.9\n",
        "        },\n",
        "    }\n",
        "\n",
        "  def resultsFor(self, s, a):\n",
        "    \"\"\"Given a state and action, returns a dictionary keyed by (state, reward)\n",
        "    pairs whose value is the probability of that pair.\n",
        "\n",
        "    This is similar to the p(s', r | s, a) function.\"\"\"\n",
        "    key = (s, a)\n",
        "    return self.p[key]\n",
        "\n",
        "  def __str__(self):\n",
        "    return str(self.p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjJmryx6-bIe"
      },
      "source": [
        "We can test the `SimpleMDP` by returning a result for a given state and action as well as printing the valid states and actions.\n",
        "\n",
        "We see that, given action `A1` from state `S1`, there's a 0.25 probability of moving to state `S1` and getting a reward of 5 and a 0.75 probability of moving to state `S2` and getting a reward of 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dCm2kUHM-sXU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "actions: ['A1', 'A2']\n",
            "states: ['S1', 'S2']\n",
            "resultsFor(\"S1\", \"A1\"): {('S1', 5): 0.25, ('S2', 0): 0.75}\n"
          ]
        }
      ],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "mdp = SimpleMDP()\n",
        "print(f'actions: {mdp.actions}')\n",
        "print(f'states: {mdp.states}')\n",
        "print(f'resultsFor(\"S1\", \"A1\"): {mdp.resultsFor(\"S1\", \"A1\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTVEbd6hOrYM"
      },
      "source": [
        "# Policy Class\n",
        "\n",
        "Next, let's define an abstract class `Policy`. The main job of a policy is to give a probability distribution over actions to take in a given state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JyoVuQabPJAy"
      },
      "outputs": [],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "class Policy:\n",
        "  def __init__(self, mdp):\n",
        "    self.mdp = mdp\n",
        "\n",
        "  def actionsFor(self, state):\n",
        "    \"\"\" Returns a dictionary keyed by actions, whose associated value\n",
        "    is the probability of choosing that action.\n",
        "\n",
        "    This is similar to the pi(a | s) function.\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\"Returns the desired action for each state.\"\"\"\n",
        "    return str({s: self.actionFor(s) for s in self.mdp.states})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTh_jPPLPJLl"
      },
      "source": [
        "# ConstantPolicy Class\n",
        "\n",
        "This class is a simple example of a policy, specifically a policy that always takes the same action, no matter what state the agent is in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "flnmIfcUOxdO"
      },
      "outputs": [],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "class ConstantPolicy(Policy):\n",
        "  def __init__(self, mdp, constantAction):\n",
        "    super().__init__(mdp)\n",
        "    self.constantAction = constantAction\n",
        "\n",
        "  def actionsFor(self, state):\n",
        "    return {self.constantAction: 1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR6NZCHz-I4c"
      },
      "source": [
        "We can test the that the `ConstantPolicy` does what it is supposed to: always return the dictionary `{constantAction: 1}`, where `constantAction` is the action given to the constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DQzm2_ts-IJV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'A1': 1}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "policy = ConstantPolicy(SimpleMDP(), \"A1\")\n",
        "policy.actionsFor(\"S1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMkNQUTUauPG"
      },
      "source": [
        "# TODO: EquiprobablePolicy Class\n",
        "\n",
        "Now you'll implement a simple policy that chooses actions with equal probability.\n",
        "\n",
        "The constructor takes an `MDP` object. You can use that to figure out what actions are possible (review the `MDP` class if you don't remember how). The `actionsFor` function should return a dictionary that contains every possible action, each with the probability 1/number of actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QvscraQFbWYI"
      },
      "outputs": [],
      "source": [
        "class EquiprobablePolicy(Policy):\n",
        "  def __init__(self, mdp):\n",
        "    super().__init__(mdp)\n",
        "\n",
        "  def actionsFor(self, state):\n",
        "    # return all the states each with a 1/n probability where n is the number of actions possible\n",
        "    equal_prob = 1/len(mdp.actions)\n",
        "    a = {}\n",
        "    for action in mdp.actions:\n",
        "      a[action] = equal_prob \n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2gg81TIeLib"
      },
      "source": [
        "Now test your `EquiprobablePolicy` class. The following should print out the dictionary `{A1: 0.5, A2: 0.5}` (though the order may vary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uW0vfK3ueftL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'A1': 0.5, 'A2': 0.5}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "policy = EquiprobablePolicy(SimpleMDP())\n",
        "policy.actionsFor(\"S1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt16JZJE_VLl"
      },
      "source": [
        "# TODO: Iterative Policy Evaluation\n",
        "\n",
        "Now, let's implement Iterative Policy Evaluation, which can be used to estimate the value function of a given policy in a given MDP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6xt7VToL9vBv"
      },
      "outputs": [],
      "source": [
        "def evaluatePolicy(mdp, pi, threshold=1e-4, v=None):\n",
        "  \"\"\"Take an MDP mdp, Policy pi and returns v, the approximation to v_pi\n",
        "     (a dictionary keyed by states, with the output of v_pi(s) as values).\n",
        "\n",
        "     The optional parameter threshold determines how\n",
        "     accurate the approximation will be.\n",
        "\n",
        "     The optional parameter v is an initial starting point\n",
        "     (possibly better than the default, which always outputs 0).\"\"\"\n",
        "  if v is None:\n",
        "    curV = {s: 0 for s in mdp.states}\n",
        "  else:\n",
        "    curV = v.copy()\n",
        "\n",
        "  delta = math.inf\n",
        "  numIterations = 0\n",
        "  while delta > threshold:\n",
        "    numIterations += 1\n",
        "    delta = 0\n",
        "    for s in mdp.states:           # sweep over all the states\n",
        "      oldValue = curV[s]           # remember the old value for comparison\n",
        "      actions = pi.actionsFor(s)   # get the distribution over actions\n",
        "      \n",
        "      \n",
        "      newVal = 0\n",
        "      for action in actions:\n",
        "        # In the MDP class the p attribute stores the p(s', r, | s, a) function\n",
        "        # Format: {(\"s\", \"a\") : {(s', r) : prob}}\n",
        "        \n",
        "        # getting pi(a | s)\n",
        "        policy_prob = actions[action]\n",
        "\n",
        "        # getting the next states and rewards associated with the current state and action\n",
        "        # cur_p = mdp.p[(s, action)]  # current (s', r) : probability\n",
        "        cur_p = mdp.resultsFor(s, action)\n",
        "\n",
        "        for val in cur_p:\n",
        "          s_prime, r = val\n",
        "          sa_prob = cur_p[val]\n",
        "\n",
        "          # increment sum\n",
        "          newVal += policy_prob * sa_prob * (r + (GAMMA * curV[s_prime]))\n",
        "\n",
        "      curV[s] = newVal \n",
        "          \n",
        "      delta = max(delta, abs(oldValue - curV[s]))\n",
        "  print(\"Evaluation Iterations: \" + str(numIterations))\n",
        "  # We've reached the accuracy threshold, so we can return\n",
        "  return curV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_HGz9GbXCPd"
      },
      "source": [
        "Let's test your policy evaluation function on our `SimpleMDP`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Wq3sXKC3L2oz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Iterations: 70\n",
            "{'S1': 2.237835524546828, 'S2': 1.7284565203872646}\n"
          ]
        }
      ],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "mdp = SimpleMDP()\n",
        "pi = EquiprobablePolicy(mdp)\n",
        "v = evaluatePolicy(mdp, pi)\n",
        "print(mdp.formatv(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEyXl09jAH7r"
      },
      "source": [
        "You should get output that matches the following (though the order may vary):\n",
        "```\n",
        "{'S1': 2.237835524546828, 'S2': 1.7284565203872646}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ERZz05pAeQt"
      },
      "source": [
        "# TODO: Gridworld\n",
        "\n",
        "Now you'll implement the Gridworld environment in Example 3.5 of the book. You'll need to complete the `resultsFor` method which needs to give the probability distribution over new states and rewards, given the current state and action.\n",
        "\n",
        "For this environment, each state will be a tuple `(x, y)` giving the agent's coordinates on the grid and the actions are the strings `\"L\"`, `\"R\"`, `\"U\"`, `\"D\"`, representing the four cardinal directions. Let the *top-left* corner represent state `(0, 0)`. Since the environment is deterministic, the returned dictionary will have only one entry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cx7aePCDSFUH"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in resultsFor\n",
        "\n",
        "class GridWorld(MDP):\n",
        "  def __init__(self):\n",
        "    self.height=5\n",
        "    self.width=5\n",
        "    super().__init__([(x, y) for x in range(self.width) for y in range(self.height)],\n",
        "                     [\"L\", \"R\", \"U\", \"D\"])\n",
        "\n",
        "  def resultsFor(self, s, a):\n",
        "    \"\"\"Given a state and action, returns a dictionary keyed by state, reward\n",
        "    pairs whose value is the probability of that pair.\n",
        "\n",
        "    This is similar to the p(s', r | s, a) function.\"\"\"\n",
        "\n",
        "    x, y = s\n",
        "\n",
        "    # Handle teleport states first\n",
        "    if s == (1, 0):\n",
        "        return {((1, 4), 10): 1}\n",
        "    elif s == (3, 0):\n",
        "        return {((3, 2), 5): 1}\n",
        "\n",
        "    newX = x\n",
        "    newY = y\n",
        "    rwd = 0\n",
        "\n",
        "    \n",
        "    if a == \"L\":\n",
        "      if x == 0:\n",
        "        # left edge of board\n",
        "        rwd = -1\n",
        "      else:\n",
        "        newX -= 1\n",
        "    elif a == \"R\":\n",
        "      if x == self.width-1:\n",
        "        rwd = -1\n",
        "      else:\n",
        "        newX += 1\n",
        "    elif a == \"U\":\n",
        "      if y == 0:\n",
        "        rwd = -1\n",
        "      else: \n",
        "        newY -= 1\n",
        "    elif a == \"D\":\n",
        "      if y == self.height - 1:\n",
        "        rwd = -1\n",
        "      else:\n",
        "        newY += 1\n",
        "\n",
        "\n",
        "    # TODO: Correctly set the values of newX, newY, and rwd\n",
        "\n",
        "    return {((newX, newY), rwd) : 1}\n",
        "\n",
        "  def formatv(self, v):\n",
        "    lines = []\n",
        "    for y in range(self.height):\n",
        "      lines.append(\" \".join([f'{v[(x, y)]:4.1f}' for x in range(self.width)]))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "  def formatpi(self, pi):\n",
        "    lines = []\n",
        "    for y in range(self.height):\n",
        "      line = \"\"\n",
        "      for x in range(self.width):\n",
        "        acts = pi.actionsFor((x, y))\n",
        "        if len(acts) == 1:\n",
        "          line += list(acts.keys())[0] + \" \"\n",
        "        else:\n",
        "          for a in acts:\n",
        "            line += a + \"(\" + str(acts[a]) + \")\"\n",
        "          line += \" | \"\n",
        "      lines.append(line)\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc61gPjHYs5w"
      },
      "source": [
        "Now you can use all of your code together to generate the value function shown in Figure 3.2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gylAvzLVxKRo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Iterations: 43\n",
            " 3.3  8.8  4.4  5.3  1.5\n",
            " 1.5  3.0  2.3  1.9  0.5\n",
            " 0.1  0.7  0.7  0.4 -0.4\n",
            "-1.0 -0.4 -0.4 -0.6 -1.2\n",
            "-1.9 -1.3 -1.2 -1.4 -2.0\n"
          ]
        }
      ],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "mdp = GridWorld()\n",
        "pi = EquiprobablePolicy(mdp)\n",
        "v = evaluatePolicy(mdp, pi)\n",
        "print(mdp.formatv(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXS5u_ZAFIYe"
      },
      "source": [
        "The results should match those in the right-hand side of Figure 3.2:\n",
        "```\n",
        " 3.3  8.8  4.4  5.3  1.5\n",
        " 1.5  3.0  2.3  1.9  0.5\n",
        " 0.1  0.7  0.7  0.4 -0.4\n",
        "-1.0 -0.4 -0.4 -0.6 -1.2\n",
        "-1.9 -1.3 -1.2 -1.4 -2.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g72kpKv4PSMg"
      },
      "source": [
        "# TODO: GreedyPolicy Class\n",
        "\n",
        "In order to improve a policy, you'll need to implement a class to represent the _greedy policy_ with respect to a given value function.\n",
        "\n",
        "Specifically, this policy is constructed from a value function $v$. In state $s$, it always chooses the action $a$ with the highest value of $q(s, a)$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned} \\pi(s) &= \\text{argmax}_a q(s, a) \\\\\n",
        "&= \\text{argmax}_a \\mathbb{E}[R_t + \\gamma v(S_t) | S_{t-1} = s, A_{t-1} = a]\\\\\n",
        "&= \\text{argmax}_a \\sum_{s', r} P(s', r | s, a)\\left[ r + \\gamma v_{\\pi}(s') \\right] \\end{aligned}\n",
        "$$\n",
        "\n",
        "In every state the greedy policy deterministically picks one action, so the dictionary returned by `actionsFor` should have only one entry that has probability 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "seDn_yw7O2wR"
      },
      "outputs": [],
      "source": [
        "class GreedyPolicy(Policy):\n",
        "  \"\"\"Policy that is greedy with respect to the given value function, v.\"\"\"\n",
        "  def __init__(self, mdp, v):\n",
        "    super().__init__(mdp)\n",
        "    self.v = v.copy()  # Copy so that changes to v don't affect us\n",
        "\n",
        "  def actionsFor(self, s):\n",
        "    maxAct = None\n",
        "    maxQ = -math.inf\n",
        "\n",
        "    # We need to calculate the Q value for each possible action given a state s\n",
        "    # Use the bellman equation for action value\n",
        "\n",
        "    # need to iterate through each possible action given our state s\n",
        "    for action in mdp.actions:\n",
        "      cur_q = 0\n",
        "      p_function = mdp.resultsFor(s, action) # the p_function has {}\n",
        "\n",
        "      # unpack s', r and the probability\n",
        "      for key in p_function:\n",
        "        s_prime, r = key\n",
        "        prob = p_function[key]\n",
        "\n",
        "        cur_q += prob * (r + (GAMMA * self.v[s_prime]))\n",
        "\n",
        "      if cur_q > maxQ:\n",
        "        maxAct = action # Updating the max action\n",
        "      maxQ = max(maxQ, cur_q) # Updating the max Q\n",
        "\n",
        "    return {maxAct : 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohYAddIFVbaz"
      },
      "source": [
        "# TODO: Policy Iteration\n",
        "\n",
        "Now we can use the `evaluatePolicy` function and the `GreedyPolicy` class within the Policy Iteration algorithm, which can find an optimal policy for our MDP!\n",
        "\n",
        "Using these two components, complete the implementation of the `policyIteration` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OlLDt3ajV7Hy"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in missing code!\n",
        "\n",
        "def policyIteration(mdp, pi, threshold=1e-4):\n",
        "  \"\"\"Takes an MDP mdp, initial policy pi and returns a tuple (vStar, piStar),\n",
        "     respectively the approximately optimal value function, and an optimal policy.\n",
        "\n",
        "     The optional parameter threshold determines how\n",
        "     accurate each policy evaluation will be.\n",
        "  \"\"\"\n",
        "  iteration = 0\n",
        "  curPi = pi\n",
        "  policyStable = False\n",
        "  while not policyStable:\n",
        "    print(\"Policy Iteration \" + str(iteration))\n",
        "    curV = evaluatePolicy(mdp, curPi, threshold)\n",
        "\n",
        "    # Printing out current value function and policy\n",
        "    # Don't change this!\n",
        "    print(\"v\")\n",
        "    print(mdp.formatv(curV))\n",
        "    print(\"\\npi\")\n",
        "    print(mdp.formatpi(curPi) + \"\\n\")\n",
        "    iteration += 1\n",
        "\n",
        "    # Create improved policy\n",
        "    nextPi = GreedyPolicy(mdp, curV)\n",
        "\n",
        "    policyStable = True\n",
        "    for state in mdp.states:\n",
        "      if curPi.actionsFor(state) != nextPi.actionsFor(state):\n",
        "        policyStable = False\n",
        "    curPi = nextPi\n",
        "    \n",
        "  piStar = curPi\n",
        "  vStar = curV\n",
        "  return (vStar, piStar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFnlXJKrRFyS"
      },
      "source": [
        "The following code will test out your policy iteration algorithm on the Gridworld example. Note that the value function and policy will be printed out at each iteration. The last value function should match the one given in Example 3.8 (page 65):\n",
        "\n",
        "```\n",
        "22.0 24.4 22.0 19.4 17.5\n",
        "19.8 22.0 19.8 17.8 16.0\n",
        "17.8 19.8 17.8 16.0 14.4\n",
        "16.0 17.8 16.0 14.4 13.0\n",
        "14.4 16.0 14.4 13.0 11.7\n",
        "```\n",
        "\n",
        "The policy you get may not exactly match that figure (since our greedy policy is deterministic) but it should be _compatible_ with it (that is, the action it chooses should be one of the optimal possibilities shown in the figure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1nsHmBnVZTlH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy Iteration 0\n",
            "Evaluation Iterations: 43\n",
            "v\n",
            " 3.3  8.8  4.4  5.3  1.5\n",
            " 1.5  3.0  2.3  1.9  0.5\n",
            " 0.1  0.7  0.7  0.4 -0.4\n",
            "-1.0 -0.4 -0.4 -0.6 -1.2\n",
            "-1.9 -1.3 -1.2 -1.4 -2.0\n",
            "\n",
            "pi\n",
            "L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | \n",
            "L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | \n",
            "L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | \n",
            "L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | \n",
            "L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | L(0.25)R(0.25)U(0.25)D(0.25) | \n",
            "\n",
            "Policy Iteration 1\n",
            "Evaluation Iterations: 36\n",
            "v\n",
            "22.0 24.4 22.0 18.4 16.6\n",
            "19.8 22.0 19.8 16.6 14.9\n",
            "17.8 19.8 17.8 14.9 13.5\n",
            "16.0 17.8 16.0 13.5 12.1\n",
            "14.4 16.0 14.4 12.1 10.9\n",
            "\n",
            "pi\n",
            "R L L L L \n",
            "U U U U L \n",
            "U U U U U \n",
            "U U U U U \n",
            "U U U U U \n",
            "\n",
            "Policy Iteration 2\n",
            "Evaluation Iterations: 24\n",
            "v\n",
            "22.0 24.4 22.0 19.4 17.5\n",
            "19.8 22.0 19.8 17.8 16.0\n",
            "17.8 19.8 17.8 16.0 14.4\n",
            "16.0 17.8 16.0 14.4 13.0\n",
            "14.4 16.0 14.4 13.0 11.7\n",
            "\n",
            "pi\n",
            "R L L L L \n",
            "R U L L L \n",
            "R U L L L \n",
            "R U L L L \n",
            "R U L L L \n",
            "\n",
            "-----------------\n",
            "\n",
            "(Approximate) v*\n",
            "22.0 24.4 22.0 19.4 17.5\n",
            "19.8 22.0 19.8 17.8 16.0\n",
            "17.8 19.8 17.8 16.0 14.4\n",
            "16.0 17.8 16.0 14.4 13.0\n",
            "14.4 16.0 14.4 13.0 11.7\n",
            "\n",
            "pi*\n",
            "R L L L L \n",
            "R U L L L \n",
            "R U L L L \n",
            "R U L L L \n",
            "R U L L L \n"
          ]
        }
      ],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "mdp = GridWorld()\n",
        "pi = EquiprobablePolicy(mdp)\n",
        "vStar, piStar = policyIteration(mdp, pi)\n",
        "print(\"-----------------\\n\")\n",
        "print(\"(Approximate) v*\")\n",
        "print(mdp.formatv(vStar))\n",
        "print(\"\\npi*\")\n",
        "print(mdp.formatpi(piStar))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477GKzH4T4ia"
      },
      "source": [
        "# TODO: Value Iteration\n",
        "\n",
        "One more thing! Now implement the Value Iteration algorithm by completing the `valueIteration` function below. It's very similar to the `evaluatePolicy` function, except that it uses the Bellman optimality equation to calculate the _optimal_ value function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NwSEC6dVV7x3"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in missing code!\n",
        "\n",
        "def valueIteration(mdp, threshold=1e-4, v=None):\n",
        "  \"\"\"Take an MDP mdp and returns a tuple (vStar, piStar),\n",
        "     respectively the approximately optimal value function and policy.\n",
        "\n",
        "     The optional parameter threshold determines how\n",
        "     accurate the approximation will be.\n",
        "\n",
        "     The optional parameter v is an initial starting point\n",
        "     (possibly better than the default, which always outputs 0).\"\"\"\n",
        "  if v is None:\n",
        "    curV = {s: 0 for s in mdp.states}\n",
        "  else:\n",
        "    curV = v.copy()\n",
        "\n",
        "  numIterations = 0\n",
        "  delta = math.inf\n",
        "  while delta > threshold:\n",
        "    numIterations += 1\n",
        "    delta = 0\n",
        "    for s in mdp.states:           # sweep over all the states\n",
        "      oldValue = curV[s]           # remember the old value for comparison\n",
        "      actions = mdp.actions        # get the actions (no probabilities needed!)\n",
        "\n",
        "      # TODO: Calculate the new value of curV[s]\n",
        "      newValue = -math.inf\n",
        "      for action in actions:\n",
        "        q=0\n",
        "        p_function = mdp.resultsFor(s, action)\n",
        "        for key in p_function:\n",
        "          s_prime, r = key\n",
        "          prob = p_function[key]\n",
        "          q += prob * (r + (GAMMA * curV[s_prime]))\n",
        "        newValue = max(newValue, q)\n",
        "      curV[s] = newValue\n",
        "\n",
        "\n",
        "      delta = max(delta, abs(oldValue - curV[s]))\n",
        "  print(\"Value Iterations: \" + str(numIterations))\n",
        "  # We've reached the accuracy threshold, so we can return\n",
        "  return (curV, GreedyPolicy(mdp, curV))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOinflAxZHGN"
      },
      "source": [
        "The following code will test out your policy iteration algorithm on the Gridworld example. The value function should match the one given in Example 3.8 (page 65):\n",
        "\n",
        "```\n",
        "22.0 24.4 22.0 19.4 17.5\n",
        "19.8 22.0 19.8 17.8 16.0\n",
        "17.8 19.8 17.8 16.0 14.4\n",
        "16.0 17.8 16.0 14.4 13.0\n",
        "14.4 16.0 14.4 13.0 11.7\n",
        "```\n",
        "\n",
        "The policy you get may not exactly match that figure (since our greedy policy is deterministic) but it should be _compatible_ with it (that is, the action it chooses should be one of the optimal possibilities shown in the figure)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eeCirwy1XtLm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value Iterations: 24\n",
            "----------------\n",
            "(Approximate) v*\n",
            "22.0 24.4 22.0 19.4 17.5\n",
            "19.8 22.0 19.8 17.8 16.0\n",
            "17.8 19.8 17.8 16.0 14.4\n",
            "16.0 17.8 16.0 14.4 13.0\n",
            "14.4 16.0 14.4 13.0 11.7\n",
            "\n",
            "pi*\n",
            "R L L L L \n",
            "R U L L L \n",
            "R U L L L \n",
            "R U L L L \n",
            "R U L L L \n"
          ]
        }
      ],
      "source": [
        "# Do not change the contents of this cell!\n",
        "\n",
        "mdp = GridWorld()\n",
        "pi = EquiprobablePolicy(mdp)\n",
        "vStar, piStar = valueIteration(mdp)\n",
        "print(\"----------------\")\n",
        "print(\"(Approximate) v*\")\n",
        "print(mdp.formatv(vStar))\n",
        "print(\"\\npi*\")\n",
        "print(mdp.formatpi(piStar))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoT5Ua4DM-4j"
      },
      "source": [
        "# TODO: Submit!\n",
        "\n",
        "1. Make sure that the output of all cells is up-to-date (easiest way is to choose _Runtime/Run all_).\n",
        "2. If you are working in Colab, choose _File/Download/Download .ipnb_ (otherwise locate your _.ipnb_ file)\n",
        "3. Go to the [Gradescope assignment](https://www.gradescope.com/courses/435656/assignments/2230644/) to submit.\n",
        "4. Choose _Upload_ as the submission method and upload your _.ipnb_ file.\n",
        "5. If you are working as a pair, make sure to push the _Group Members_ button to add the other partner."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
